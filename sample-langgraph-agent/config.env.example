# Defaults when using ollama with llama stack
LLAMA_STACK_SERVER_OPENAI=http://localhost:8321/v1/openai/v1
INFERENCE_MODEL=ollama/llama3.2:3b-instruct-fp16

# When using openai with llama stack
#LLAMA_STACK_SERVER_OPENAI=http://localhost:8321/v1/openai/v1
#INFERENCE_MODEL=openai/gpt-4o-mini

# When using openai directly
# LLAMA_STACK_SERVER_OPENAI=https://api.openai.com/v1
# INFERENCE_MODEL=gpt-4o-mini